{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinetics I\n",
    "\n",
    "Here, we will get into kinetic analysis.  This lecture covers differential analysis of data collected in a constant volume batch reactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from math import ceil, floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Problem 01\n",
    "\n",
    "Analysis of Data Collected in a Constant Volume Batch Reactor.\n",
    "\n",
    "The following reaction is carried out in a well-mixed, constant volume batch reactor. The concentration of species A in the tank is initially 10M, and the fluid inside of the reactor has constant density. \n",
    "\t\t\t\n",
    "$$A \\longrightarrow B$$\n",
    "\t\t\t\n",
    "You measure the concentration of species A in this reactor, monitoring how it changes with time.  The data you collect are given in the table below. \n",
    "\n",
    "|**time (min)**| **C$_A$ (mol/L)** | **time (min)** | **C$_A$ (mol/L)**      |\n",
    "|:------------:|:-----------------:|:--------------:|:----------------------:|\n",
    "|0             | 10.0              | 10             | 3.68                   |\n",
    "|1             | 9.05              | 12             | 3.01                   |\n",
    "|2             | 8.19              | 15             | 2.23                   |\n",
    "|3             | 7.41              | 20             | 1.35                   |\n",
    "|4             | 6.70              | 25             | 0.821                  | \n",
    "|5             | 6.07              | 30             | 0.498                  |\n",
    "|6             | 5.49              | 45             | 0.111                  |\n",
    "|7             | 4.97              | 60             | 0.0248                 |\n",
    "|8             | 4.49              | 90             | 0.00123                |\n",
    "|9             | 4.07              | 120            | 6.14 $\\times$ 10$^{-5}$|\n",
    "\n",
    "\n",
    "Assuming the rate law is described by power law kinetics,\n",
    "\n",
    "$$r = kC_A^{\\alpha}$$ \n",
    "\n",
    "where $\\alpha$ is an integer, determine the reaction order in A and the rate constant for this reaction.\n",
    "\n",
    "In the cell below, we compile this set of measurements.  In this type of system, it is common to start the system with a known quantity of total moles and a known concentration of reacting species.  Then we allow time to proceed, and we measure the concentrations of reactants and products as a function of time.\n",
    "\n",
    "\n",
    "We know the following:\n",
    "\n",
    "\\begin{align}\n",
    "    C_{A0} &= 10 \\ \\textrm{mol} \\ \\textrm{L}^{-1} \\\\\n",
    "    C_{B0} &= 0 \\ \\textrm{mol} \\ \\textrm{L}^{-1} \\\\\n",
    "    T &= \\textrm{constant}\n",
    "\\end{align}\n",
    "\n",
    "Usually, when we have batch data, we're looking at something like this...where we've monitored the concentration of A in the reactor as time progresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20, 25, 30, 45, 60, 90, 120]) #time in minutes\n",
    "CA = np.array([10, 9.05, 8.19, 7.41, 6.70, 6.07, 5.49, 4.97, 4.49, 4.07, 3.68, 3.01, 2.23, 1.35, 0.821, 0.498, 0.111, 0.0248, 1.23E-3, 6.14E-05])\n",
    "#Concentrations in moles per liter\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(t, CA, edgecolor = 'black', color = 'none', label = 'CA')\n",
    "plt.xlabel('time (min)', fontsize = 12)\n",
    "plt.ylabel('CA (M)', fontsize = 12)\n",
    "plt.xlim(0, 120)\n",
    "plt.ylim(0, 12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the rate law and kinetic parameters $(k, \\alpha)$ for this reaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Example Problem 01\n",
    "\n",
    "This is our introduction to an \"inverse problem,\" which involves determining a model and the parameters for that model based on our observations of data. To this point, we've only used models (material balances, rate laws, etc.) in order to predict data (e.g., reactor volume required for a certain conversion).  Here, we have to work backwards and use the data to develop the correct model. Extracting information from data can be challenging, but we'll work through a couple of methods and highlight some of the pifalls associated with each one to build experience.\n",
    "\n",
    "In this case, there is no reason to believe that this reaction is an elementary step, so, at best, we can propose that the rate of reaction depends on temperature (which we capture with a rate constant) and the concentration of the reactant, A, with some unknown order, i.e., we'll propose a power law model:\n",
    "\n",
    "$$r = k{C_A}^\\alpha$$\n",
    "\n",
    "Now we have to make some decisions about how we'll estimate those parameters $(k, \\alpha)$.  It all starts with the material balance on a constant volume batch reactor.  We'll write one on species A:\n",
    "\n",
    "$$\\frac{dN_A}{dt} = R_AV$$\n",
    "\n",
    "For a constant volume reactor, we can divide through by V and move it inside of the derivative:\n",
    "\n",
    "$$\\frac{dC_A}{dt} = R_A$$\n",
    "\n",
    "And for a single reaction, we know that $R_A = -r$, so:\n",
    "\n",
    "$$\\frac{dC_A}{dt} = -r$$\n",
    "\n",
    "We've proposed power law kinetics here, hence:\n",
    "\n",
    "$$\\frac{dC_A}{dt} = -k{C_A}^\\alpha$$\n",
    "\n",
    "Now we have options.  We can generally choose one of two strategies to approach the problem.  The first is called a ***differential*** method of analysis, wherein we attempt to approximate reaction rates by taking numerical derivatives of data.  The second is an ***integral*** method of analysis, where we attempt to solve the material balance and then regress parameters in the resultant model that best describe our data. Ideally, you'll want to use both of them as each has advantages and disadvantages for the analysis of batch reactor data.\n",
    "\n",
    "Generally, an ***integral method of analysis*** is easiest if we are able to assume a reaction order (1, 2, 3...).  Once we do that, we know we can solve the differential equation symbolically in terms of $k$ and $C_{A0}$. The hard part about this is there is no guarantee that our reaction will be integer or even positive order with respect to a particular variable, so it can be a bit tedious to consider many different reaction orders.\n",
    "\n",
    "A ***differential method of analysis*** is fast to apply, and it lets us estimate reaction orders and rate constants without solving a differential equation, but taking numerical derivatives is very imprecise and this method is prone to a lot of error, which can make it very difficult to apply to real world data.\n",
    "\n",
    "In reality, both methods have strengths and weaknesses, and I always use both of them to give me a complete picture of the system under consideration.  Usually, a combination of differential and integral methods will allow us to get a pretty good idea of the reaction order and any kinetic parameters we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Differential Analysis of Batch Reactor Data\n",
    "\n",
    "#### Finite Differences\n",
    "\n",
    "We'll start with a differential method of analysis.  This is based on approximating derivatives with a finite difference method.  This is really straightforward despite the fancy name.  We simply say that we can approximate derivatives from data using the discrete changes in that data that we observe:\n",
    "\n",
    "$$\\frac{dC_A}{dt} \\approx \\frac{\\Delta C_A}{\\Delta t}$$\n",
    "\n",
    "And that is convenient because, ***in a constant volume batch reactor***, we know the relationship between the concentration derivative and the reaction rate, namely:\n",
    "\n",
    "$$\\frac{dC_A}{dt} = -r$$\n",
    "\n",
    "So, by approximating the time derivative of concentration in a constant volume batch reactor, we are actually getting a reasonable approximation of the reaction rate at a certain condition.\n",
    "\n",
    "$$\\frac{\\Delta C_A}{\\Delta t} \\approx -r$$\n",
    "\n",
    "Let's start there, if we can take those finite differences, we'll get an estimate of reaction rates.  I am specifically going to use a forward difference method here.  What this approximation says is that:\n",
    "\n",
    "$$\\frac{dC_A}{dt}\\bigg|_{C_{A_n}} \\approx \\frac{C_{A_{n+1}} - C_{A_{n}}}{t_{n+1} - t_n}$$\n",
    "\n",
    "I can quickly find differences between elements in an array using `numpy.diff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ΔCA = np.diff(CA)\n",
    "print(ΔCA, '\\n')\n",
    "Δt  = np.diff(t)\n",
    "print(Δt, '\\n')\n",
    "r = -1*ΔCA/Δt\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is a pretty quick way to convert a concentration vs. time profile into approximate (sometimes very approximate) reaction rates.  As indicated by our definition of the forward difference, these entries represent the derivative at point \"$n$\".  So the first rate is the rate at `CA[0]`, the second rate is the rate at `CA[1]`, etc.  The only catch is that I have 20 measurements. I can only get 19 derivatives out of 20 measurements based on the forward difference formula.  So the final value in our rate array represents an estimate of the reaction rate at `CA[18]`, i.e., the 19th concentration measurement.  In this case, we don't have an estimate for the rate at `CA[19]` because we are using a forward difference algorithm at it would require us to know the 21st concentration, which we didn't measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(CA), '\\n')\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing rate as a function of CA\n",
    "\n",
    "Remember: we have a model for how rate should vary with concentration:\n",
    "\n",
    "$$r = k{C_A}^\\alpha$$\n",
    "\n",
    "Let's plot rate against $C_A$ and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cmod = CA[:-1] #keep all but last CA entry\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(Cmod, r, edgecolor = 'black', color = 'none')\n",
    "plt.xlabel('CA (M)', fontsize = 12)\n",
    "plt.ylabel('rate (mol/L/min)', fontsize = 12)\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting Linearization of rate vs. CA data\n",
    "\n",
    "Well, OK...looks kind of like rate increases linearly with concentration.  But I'd like to be a bit more quantitative in specifying the reaction order.  We can do this by linearizing the model and regressing a slope and intercept, which we can relate directly to the reaction order and rate constant.  Specifically, we know:\n",
    "\n",
    "$$r = k{C_A}^\\alpha$$\n",
    "\n",
    "If we take logarithms of each side (i'll use natural logarithms here), we can linearize the model:\n",
    "\n",
    "$$\\ln{r} = \\alpha\\ln{C_A} + \\ln{k}$$\n",
    "\n",
    "It may not look like it at first glance, but that is just a normal, $y = mx + b$ model, where our slope gives the reaction order $\\alpha$, and the y-intercept gives the logarithm of the rate constant.  So, if we plot $\\ln{r}$ on the y-axis against $\\ln{C_A}$ on the x-axis, we should see a straight line if our power law model holds.  Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(Cmod, r, edgecolor = 'black', color = 'none')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('CA(M)', fontsize = 12)\n",
    "plt.ylabel('rate (mol/L/min)', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing linearity; linear regression\n",
    "\n",
    "Is that linear? I really can't tell.  It looks like parts of it are. The best way to tell is for me to fit a line to the data.  Clearly, there are 19 rate vs. CA pairs, so I can't find a single line that goes through all of them.  I can, however, regress the best fit line using any number of methods we discussed during recitation.  I can create a Vandermonde matrix and use linear algebra.  I can use `np.polyfit()`.  Or I can use the least squares version of `np.linalg.solve()`, which is called `np.linalg.lstsq()`.  All give the exact same slope and intercept values.  We'll discuss more what these various tools are doing during recitation this week.  For now, consider them similar to Excel's trendline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = np.log(r)\n",
    "xdata = np.log(Cmod)\n",
    "\n",
    "X     = np.vander(xdata, 2)#This creates the X matrix shown above, truncating at 1st order\n",
    "Y     = ydata    \n",
    "A     = np.linalg.solve(X.T@X, X.T@Y) \n",
    "Ypred = X@A\n",
    "Ybar  = np.mean(ydata)\n",
    "Ndata = len(ydata)\n",
    "SSE   = (Y - X@A).T@(Y - X@A) #Residual sum of squares, equivalent to SSE = np.sum((Y - Ypred)**2)\n",
    "SST   = np.sum((Y - Ybar)**2) #Total sum of squares\n",
    "MSE   = SSE/Ndata             #Mean Square Error\n",
    "RMSE  = np.sqrt(MSE)          #Root Mean Square Error\n",
    "R2    = 1 - SSE/SST           #R^2\n",
    "DOF   = len(ydata) - len(A)   #Degrees of freedom = number of measurements - number of regressed parameters\n",
    "s2    = SSE/DOF               #Estimate of variance, s2 ~ σ2  \n",
    "COV   = s2*np.linalg.inv(X.T@X) #Covariance matrix, use for standard errors and conf. intervals\n",
    "m     = A[0]                  #slope of best fit line\n",
    "b     = A[1]                  #y-intercept of best fit line\n",
    "SEm   = np.sqrt(COV[0, 0])    #Standard error in slope\n",
    "SEb   = np.sqrt(COV[1, 1])    #Standard error in y-intercept\n",
    "tval  = stats.t.ppf(0.975, DOF) #t distribution value for 95% confidence intervals\n",
    "CIm   = SEm*tval              #95% confidence interval on slope\n",
    "CIb   = SEb*tval              #95% confidence interval on y-intercept\n",
    "α     = m\n",
    "k     = np.exp(b)\n",
    "labels = ['m', 'b', 'SSE', 'SST','MSE','RMSE', 'R2', 'α', 'k']\n",
    "values = [m  ,  b ,  SSE ,  SST , MSE , RMSE ,  R2, α, k]\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "for label, value in zip(labels, values):\n",
    "    if label == 'm':\n",
    "        print(f'{label:4s} = {value:0.2f} +/- {CIm:0.2f}')\n",
    "    elif label == 'b':\n",
    "        print(f'{label:4s} = {value:0.2f} +/- {CIb:0.2f}')\n",
    "    else:\n",
    "        print(f'{label:4s} = {value:0.2f}')\n",
    "\n",
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.scatter(X[:, 0], Y, marker = 'o', color = 'none', edgecolor = 'black', label = 'experimental rates')\n",
    "plt.plot(X[:, 0], Ypred, color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'linear fit')\n",
    "plt.xlabel('ln(CA) mmol/L', fontsize = 12)\n",
    "plt.ylabel('ln(r) mmol/L/min', fontsize = 12)\n",
    "# plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "# plt.ylim(floor(min(Y)), ceil(max(Y)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize = (5, 5))\n",
    "# plt.scatter(X[:, 0], (Y - Ypred), marker = 'o', color = 'none', edgecolor = 'black', label = 'residual error')\n",
    "# plt.hlines(0, floor(min(X[:, 0])), ceil(max(X[:,0])), color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'zero error')\n",
    "# plt.xlabel('ln (CA) mmol/L', fontsize = 12)\n",
    "# plt.ylabel('Residual Error in ln (r) (mmol/L/min)', fontsize = 12)\n",
    "# plt.title('Residual Plot')\n",
    "# plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "# plt.legend(loc = 'lower center')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so it looks *sort of* linear...let's see what the slope and intercept are...this should tell us the reaction order and rate constant, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α     = m\n",
    "k     = np.exp(b)\n",
    "print(f'α = {α:3.2f}') \n",
    "print(f'k = {k:3.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so according to that analysis, this is a 1.15 order reaction and the rate constant is 0.072 inverse minutes (or so).\n",
    "\n",
    "The problem is: I don't buy it.  Now is the time to tell you that I generated this data by simulating a batch reactor with the following information:\n",
    "\n",
    "\\begin{align}\n",
    "    r & = kC_A \\\\\n",
    "    k &= 0.1 \\ \\textrm{min}^{-1} \\\\\n",
    "    C_{A0} &= 10 \\ \\textrm{mol} \\ \\textrm{L}^{-1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In other words, I generated this by simulating a 100% first order reaction, and I don't like that I'm getting 1.15 order from my differential analysis.  This is actually a common problem when using differential analysis, which is why I chose to highlight it.  The fundamental problem is that we are treating our rates as if we actually measured them, but remember: we estimated them using a finite difference method (similar to an Euler step...):\n",
    "\n",
    "$$r|_{C_{A_n}} = \\frac{dC_A}{dt}\\bigg|_{C_{A_n}} \\approx \\frac{C_{A_{n+1}} - C_{A_{n}}}{t_{n+1} - t_n}$$\n",
    "\n",
    "That approximation basically assumes that the derivative of concentration with respect to time is constant between $t_n$ and $t_{n+1}$. That can be a pretty good approximation if we consider small changes in time and concentration. That approximation can actually be pretty bad if we assume it is true over large changes in time and/or concentration.  So if we look back at our raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(t, CA, edgecolor = 'black', color = 'none', label = 'CA')\n",
    "plt.xlabel('time (min)', fontsize = 12)\n",
    "plt.ylabel('CA (M)', fontsize = 12)\n",
    "plt.xlim(0, 125)\n",
    "plt.ylim(0, 12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It kind of looks like we made some bad decisions.  That forward difference is probably a good approximation for derivatives/rates below about 10 - 15 minutes, where we were sampling frequently, and we are considering fairly small changes in concentration and time.  Beyond about 20 minutes, the time steps start to get larger between our measurments...and it is very unlikely we can treat the derivative as constant over that entire range.  I know it is hard to see on a log scale, but if we go back to that linearized form (our plot of ln(rate) vs ln(CA)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(X[:, 0], Y, marker = 'o', color = 'none', edgecolor = 'black', label = 'experimental rates')\n",
    "plt.plot(X[:, 0], Ypred, color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'linear fit')\n",
    "plt.xlabel('ln(CA) mmol/L', fontsize = 12)\n",
    "plt.ylabel('ln(r) mmol/L/min', fontsize = 12)\n",
    "# plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "# plt.ylim(floor(min(Y)), ceil(max(Y)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data looks pretty linear between ln(CA) from about 1 to a bit more than 2.  So, those are high concentrations of A.  Things get a bit chaotic when we look at ln(CA) below about 1, where we start to see a lot more deviation and scatter.  Those are the low concentrations of A, and we have really poor estimates of the rate here because we applied a finite difference over a very large time step (30 minutes, 45 minutes), etc. Another way to understand this is to look at a plot of $(y_i - \\hat{y}_i)$ vs. $ln(C_A)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(X[:, 0], (Y - Ypred), marker = 'o', color = 'none', edgecolor = 'black', label = 'residual error')\n",
    "plt.hlines(0, floor(min(X[:, 0])), ceil(max(X[:,0])), color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'zero error')\n",
    "plt.xlabel('ln (CA) mmol/L', fontsize = 12)\n",
    "plt.ylabel('Residual Error in ln (r) (mmol/L/min)', fontsize = 12)\n",
    "plt.title('Residual Plot')\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "plt.legend(loc = 'upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our data in the CA vs. time graph is actually a perfect data set--I generated it by simulation.  Yet because we've carelessly applied finite differences and treated them as actual derivatives, we've gotten ourselves into a confusing situation.  What should we do?  In this case, I would recommend that we extract the linear portion of the ln(r) vs. ln(CA) plot.  I suggest we do this with a justifiable method, not just throwing out noisy data.  I want to retain all data where the time step was 1 minute or less, because at that point, I see that my concentrations are changing by less than 10% with every measurement.  That is enough to be experimentally distinguishable, but not so much that my finite difference is a terrible approximation.  I can prove that to myself by looking at the `np.diff()` of CA divided by CA--this gives me percent changes, and you can see where it goes to > 10% at about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = np.diff(CA)/(Cmod)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(t[:-1], changes, edgecolor = 'black', color = 'none')\n",
    "plt.xticks(np.linspace(0, 90, 10))\n",
    "plt.xlim(0, 90)\n",
    "plt.xlabel('time (min)')\n",
    "plt.ylabel('ΔCA (M)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, I can justify that the data obtained above 10 minutes are not reliable ***in the context of a differential analysis*** (they are perfect data otherwise, so do not throw them out!).  It's just that the time steps are too large for a finite difference to be a good approximation.  Let's trim the data set and re-do our differential analysis.  I'm only going to keep derivative approximations for concentrations measured at times = 0 to 9 minutes.  I'm doing this because, to get the derivative estimate at 9 minutes, I use the data at 9 minutes and the data at 10 minutes.  To the get the esimtate at 10 minutes, I'd need to use the data at 10 minutes and the next point at 12 minutes, which we've determined has an unacceptably large change in time/concentration for the finite difference to be reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnew  = t[0:10] \n",
    "CAnew = CA[0:10]\n",
    "print(np.array([tnew, CAnew]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we change the data we're regressing, let's take a look at the quantitative metrics on our regression analysis so that we can determine whether we can improve them by removing data that are not appropriate for a differential analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, value in zip(labels, values):\n",
    "    if label == 'm':\n",
    "        print(f'{label:4s} = {value:0.4f} +/- {CIm:0.4f}')\n",
    "    elif label == 'b':\n",
    "        print(f'{label:4s} = {value:0.4f} +/- {CIb:0.4f}')\n",
    "    else:\n",
    "        print(f'{label:4s} = {value:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata  = np.log(r[0:10])\n",
    "xdata  = np.log(CA[0:10])\n",
    "X      = np.vander(xdata, 2)#This creates the X matrix shown above, truncating at 1st order\n",
    "Y      = ydata    \n",
    "A      = np.linalg.solve(X.T@X, X.T@Y) \n",
    "Ypred  = X@A\n",
    "Ybar   = np.mean(ydata)\n",
    "Ndata  = len(ydata)\n",
    "SSE    = (Y - X@A).T@(Y - X@A) #Residual sum of squares, equivalent to SSE = np.sum((Y - Ypred)**2)\n",
    "SST    = np.sum((Y - Ybar)**2) #Total sum of squares\n",
    "MSE    = SSE/Ndata             #Mean Square Error\n",
    "RMSE   = np.sqrt(MSE)          #Root Mean Square Error\n",
    "R2     = 1 - SSE/SST           #R^2\n",
    "DOF    = len(ydata) - len(A)   #Degrees of freedom = number of measurements - number of regressed parameters\n",
    "s2     = SSE/DOF               #Estimate of variance, s2 ~ σ2  \n",
    "COV    = s2*np.linalg.inv(X.T@X) #Covariance matrix, use for standard errors and conf. intervals\n",
    "m      = A[0]                  #slope of best fit line\n",
    "b      = A[1]                  #y-intercept of best fit line\n",
    "SEm    = np.sqrt(COV[0, 0])    #Standard error in slope\n",
    "SEb    = np.sqrt(COV[1, 1])    #Standard error in y-intercept\n",
    "tval   = stats.t.ppf(0.975, DOF) #t distribution value for 95% confidence intervals\n",
    "CIm    = SEm*tval              #95% confidence interval on slope\n",
    "CIb    = SEb*tval              #95% confidence interval on y-intercept\n",
    "α      = m\n",
    "k      = np.exp(b)\n",
    "labels = ['m', 'b', 'SSE', 'SST','MSE','RMSE', 'R2', 'α', 'k']\n",
    "values = [m  ,  b ,  SSE ,  SST , MSE , RMSE ,  R2, α, k]\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "for label, value in zip(labels, values):\n",
    "    if label == 'm':\n",
    "        print(f'{label:4s} = {value:0.4f} +/- {CIm:0.4f}')\n",
    "    elif label == 'b':\n",
    "        print(f'{label:4s} = {value:0.4f} +/- {CIb:0.4f}')\n",
    "    else:\n",
    "        print(f'{label:4s} = {value:0.4f}')\n",
    "\n",
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.scatter(X[:, 0], Y, marker = 'o', color = 'none', edgecolor = 'black', label = 'experimental rates')\n",
    "plt.plot(X[:, 0], Ypred, color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'linear fit')\n",
    "plt.xlabel('ln(CA) mmol/L', fontsize = 12)\n",
    "plt.ylabel('ln(r) mmol/L/min', fontsize = 12)\n",
    "# plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "# plt.ylim(floor(min(Y)), ceil(max(Y)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize = (5, 5))\n",
    "plt.scatter(X[:, 0], (Y - Ypred), marker = 'o', color = 'none', edgecolor = 'black', label = 'residual error')\n",
    "plt.hlines(0, floor(min(X[:, 0])), ceil(max(X[:,0])), color = 'blue', linestyle = 'dashed', linewidth = 1, label = 'zero error')\n",
    "plt.xlabel('ln (CA) mmol/L', fontsize = 12)\n",
    "plt.ylabel('Residual Error in ln (r) (mmol/L/min)', fontsize = 12)\n",
    "plt.title('Residual Plot')\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.xlim(floor(min(X[:, 0])), ceil(max(X[:,0])))\n",
    "plt.legend(loc = 'lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I note that the order and rate constant are *much closer* to their true values.  They still aren't exact because this whole method is based on finite difference *approximations* to derivatives, but they are pretty good!! Second, we overlay the model and linearized data set to see how it looks; visual analysis is great for assessing model fit.  This actually does a great job, and we reasoned our way through which data should and should not be used for a differential analysis.  You can see that finite differences cause problems even for perfect data.  A differential analysis is great, but you do have to beward the limits of precision you run into with the method.\n",
    "\n",
    "Finally, it is worth knowing that much of the above linear/polynomial regression and prediction is automated in `numpy` through `polyfit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata2 = np.log(CA[0:10])\n",
    "ydata2 = np.log(r[0:10])\n",
    "A4     = np.polyfit(xdata2, ydata2, 1)\n",
    "print(A4)\n",
    "order = A4[0]\n",
    "k     = np.exp(A4[1])\n",
    "print(order, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.polyval(A4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.scatter(xdata2, ydata2, edgecolor = 'black', color = 'none', label = 'experiment')\n",
    "plt.plot(xdata2, np.polyval(A4, xdata2), color = 'black', linestyle = 'dashed', linewidth = 0.75, label = 'regressed model')\n",
    "plt.xlabel('ln(CA(M))', fontsize = 12)\n",
    "plt.ylabel('ln(rate (mol/L/min))', fontsize = 12)\n",
    "plt.legend(fontsize = 9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0ZJPj0jWZ7tx7s+T6+2CS",
   "name": "P14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
